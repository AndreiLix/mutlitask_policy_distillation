# Addressing Task Interference through Multitask Policy Distillation

This thesis project compared the stability of standard multitask learning techniques with a novel algorithm for multitask policy distillation (MTPD). The results show that MTPD achieves greater stability across learning, with a lower sensitivity to initializations.


![image](https://github.com/AndreiLix/mutlitask_policy_distillation/assets/94043928/b88bdad9-dfef-4410-a252-86eb5235ecf6)



Below lies the thesis poster, containing the goals at th einception of the project;

![Thesis poster final_page-0001 (1)](https://github.com/AndreiLix/mutlitask_policy_distillation/assets/94043928/b1d0549a-9c6e-4f3a-b092-fae0f63f06de)
